In this project, I used a wine data set from Kaggle to learn as much as I could about classification machine learning models and different feature engineering/selection techniques. 

First, I cleaned the data and classified the 'quality' feature into 'good' and 'bad' wines. I then ran the machine learning models that I wanted to use without any feature engineering or selectionâ€”this was my control group. After, I dove deeper into each machine learning model and did feature engineering and/or selection to see if I could improve upon the accuracy score of the control group.

I do wish to note: I did not do this to perfectly optimize each model. I did this so that I could learn about various feature engineering/selection techniques and practice implementing them on the dataset. I know I could have combined and, generally, improved upon each model further, but that was not my goal.

The models I used were: Logistic Regression, K-Nearest Neighbors, Gaussian Naive Bayes, Support Vector Classification, Random Forest Classifier, and Decision Tree Classifier.

There are a LOT of improvements to be made and many more concepts to be learned. However, I learned a lot from this project and really enjoyed figuring out each step of the process.
